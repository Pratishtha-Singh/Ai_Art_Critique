{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a0cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be73217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254a8bc1c81f493ab0be8220859ec46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and processor for image embeddings\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text model and tokenizer for text embeddings\n",
    "text_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45b83fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape: torch.Size([1, 128])\n",
      "Sample text: acolman-1-1955 I like the woods and as this looks like a graveyard, it soothes. I like the woods and\n"
     ]
    }
   ],
   "source": [
    "# Load sample data and generate embeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Load from local unified dataset\n",
    "df = pd.read_csv('../data/03_primary/unified_art_dataset.csv')\n",
    "sample = df.iloc[0]\n",
    "\n",
    "# Generate text embedding\n",
    "text = sample.get('title', '') + \" \" + sample.get('utterance', '') + \" \" + sample.get('emotion_text', '')\n",
    "text_inputs = text_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    text_embedding = text_model(**text_inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Text embedding shape:\", text_embedding.shape)\n",
    "print(\"Sample text:\", text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to download image from: https://uploads6.wikiart.org/images/aaron-siskind/acolman-1-1955.jpg\n",
      "Response status: 200\n",
      "Image downloaded successfully\n",
      "Image embedding shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Batch generate text embeddings for 10k samples\n",
    "import pandas as pd\n",
    "\n",
    "# Load from local unified dataset\n",
    "df = pd.read_csv('../data/03_primary/unified_art_dataset.csv')\n",
    "df_subset = df.head(10000).copy()  # 10k samples\n",
    "\n",
    "text_embeddings = []\n",
    "for idx, row in df_subset.iterrows():\n",
    "    text = f\"{row.get('title', '')} {row.get('utterance', '')} {row.get('emotion_text', '')}\".strip()\n",
    "    if not text:\n",
    "        text = \"No description available\"\n",
    "    \n",
    "    inputs = text_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embedding = text_model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    text_embeddings.append(embedding)\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1} text embeddings\")\n",
    "\n",
    "df_subset['text_embedding'] = text_embeddings\n",
    "print(f\"Generated {len(text_embeddings)} text embeddings\")\n",
    "\n",
    "# Save text embedded dataset\n",
    "df_subset.to_csv('../data/04_feature/text_embedded_art_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load dataset (assuming text-embedded dataset)\n",
    "df = pd.read_csv('data/04_feature/text_embedded_art_dataset.csv')\n",
    "\n",
    "# Filter for high-repetition artworks\n",
    "repetition_counts = df.groupby('image_name').size()\n",
    "high_rep_artworks = repetition_counts[repetition_counts >= 5].index\n",
    "filtered_df = df[df['image_name'].isin(high_rep_artworks)]\n",
    "\n",
    "# Select diverse 1k artworks\n",
    "filtered_df = filtered_df.sort_values(['style', 'artist']).drop_duplicates(subset=['image_name']).head(1000)\n",
    "\n",
    "embeddings = []\n",
    "successful_rows = []\n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    image_name = row.get('image_name', '')\n",
    "    if image_name:\n",
    "        # Construct URL\n",
    "        parts = image_name.split('/')\n",
    "        if len(parts) == 2:\n",
    "            artist_title = parts[1].replace('_', '/').replace('.jpg', '')\n",
    "            artist, title = artist_title.split('/', 1)\n",
    "            image_url = f\"https://uploads6.wikiart.org/images/{artist}/{title}.jpg\"\n",
    "        else:\n",
    "            image_url = f\"https://uploads6.wikiart.org/images/{image_name}\"\n",
    "        \n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                image = Image.open(response.raw)\n",
    "                inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    embedding = clip_model.get_image_features(**inputs).squeeze().numpy()\n",
    "                embeddings.append(embedding)\n",
    "                successful_rows.append(row)\n",
    "                if len(successful_rows) % 10 == 0:\n",
    "                    print(f\"Processed {len(successful_rows)} image embeddings\")\n",
    "            else:\n",
    "                print(f\"Failed download: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"No image_name\")\n",
    "\n",
    "# Save results\n",
    "final_df = pd.DataFrame(successful_rows)\n",
    "final_df['image_embedding'] = embeddings\n",
    "final_df.to_csv('data/04_feature/embedded_art_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
